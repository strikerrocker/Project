{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2352b5d93f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the data transformations (you can customize this based on your data)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to fit AlexNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Assuming you have a 'train' folder with subfolders for each class in your dataset\n",
    "train_dataset = datasets.ImageFolder(root='../Dataset/Cropped', transform=data_transforms)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the data loader\n",
    "dataloaders = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / 928\n",
    "                epoch_acc = running_corrects.double() / 928\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Conda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Programs\\Conda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "num_ftrs = alexnet.classifier[6].in_features\n",
    "alexnet.classifier[6] = nn.Linear(num_ftrs, 4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_conv = alexnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "optimizer_conv = optim.Adam(model_conv.classifier[6].parameters(), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduling (optional)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.3910 Acc: 0.3254\n",
      "val Loss: 1.2602 Acc: 0.4321\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.2313 Acc: 0.4709\n",
      "val Loss: 1.1642 Acc: 0.5183\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.1904 Acc: 0.4591\n",
      "val Loss: 1.1029 Acc: 0.5377\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.1173 Acc: 0.5420\n",
      "val Loss: 1.0621 Acc: 0.5787\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.0718 Acc: 0.5636\n",
      "val Loss: 1.0339 Acc: 0.5873\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.0545 Acc: 0.5722\n",
      "val Loss: 0.9990 Acc: 0.6433\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.0317 Acc: 0.5991\n",
      "val Loss: 0.9809 Acc: 0.6239\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.0119 Acc: 0.5884\n",
      "val Loss: 0.9755 Acc: 0.6293\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.0053 Acc: 0.6056\n",
      "val Loss: 0.9717 Acc: 0.6369\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.0143 Acc: 0.5894\n",
      "val Loss: 0.9693 Acc: 0.6422\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.0102 Acc: 0.6024\n",
      "val Loss: 0.9678 Acc: 0.6390\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.0114 Acc: 0.5787\n",
      "val Loss: 0.9658 Acc: 0.6466\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.0075 Acc: 0.5981\n",
      "val Loss: 0.9637 Acc: 0.6466\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.0224 Acc: 0.5830\n",
      "val Loss: 0.9617 Acc: 0.6487\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.9903 Acc: 0.6078\n",
      "val Loss: 0.9615 Acc: 0.6487\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.0029 Acc: 0.6024\n",
      "val Loss: 0.9613 Acc: 0.6487\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.0170 Acc: 0.5862\n",
      "val Loss: 0.9611 Acc: 0.6487\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.0213 Acc: 0.5765\n",
      "val Loss: 0.9609 Acc: 0.6487\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.0187 Acc: 0.5841\n",
      "val Loss: 0.9607 Acc: 0.6487\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.0223 Acc: 0.5744\n",
      "val Loss: 0.9606 Acc: 0.6487\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.0163 Acc: 0.5873\n",
      "val Loss: 0.9604 Acc: 0.6487\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.0083 Acc: 0.6013\n",
      "val Loss: 0.9604 Acc: 0.6487\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.0012 Acc: 0.6002\n",
      "val Loss: 0.9603 Acc: 0.6487\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.0145 Acc: 0.5873\n",
      "val Loss: 0.9603 Acc: 0.6487\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.0025 Acc: 0.6002\n",
      "val Loss: 0.9603 Acc: 0.6487\n",
      "\n",
      "Training complete in 9m 52s\n",
      "Best val Acc: 0.648707\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b1e95b2ca5150e092e4c21df16422a1db35cdf8311566f29b0b2b2feeca4539"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
